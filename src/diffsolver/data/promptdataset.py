import os
import shutil

import gdown
import pandas as pd
import torch
import tqdm
from safetensors.torch import safe_open, save_file
from torch.utils.data import Dataset

from diffsolver.utils import update_con_decon
from .data_prepare import FlowEditFLUX
from .unlearn_template import (  # noqa F401
    CON_DECON_DICT,
    NSFW_TEMPLATES,
    SIMPLE_DECONCEPT_TEMPLATES,
    SIMPLE_DESTYLE_TEMPLATES,
    SYNONYMS_DICT,
)

METADICT = {"gcc": "https://drive.google.com/file/d/1VCWJ9YeLwqbT_TyvdV_aZWp0qkpHdEkz/view?usp=sharing"}


CON_DECON_DICT = update_con_decon(con_decon_dict=CON_DECON_DICT, synonyms_dict=SYNONYMS_DICT)


class PromptImageDataset(Dataset):
    """
    A dataset for generating prompted images from different metadata, such as gcc3m, gcc12m, yfcc, laion400m, etc.
    store the image in the tmp folder and return the path of the image.

    # Arguments
    metadata: str, path to the metadata file, can be a tsv file or a yaml file, for pruningt
    while perserving the image quality
    deconceptmeta: str, path to the deconcept metadata file

    """

    def __init__(
        self,
        metadata,
        deconceptmeta,
        pipe,
        num_inference_steps,
        save_dir,
        seed,
        device,
        size=45,
        concept=None,
        neutral_concept=None,
        only_deconcept_latent=False,  # only use deconcept latent for training
        keep_old=False,
        style=True,
        num_saved_latents=2,  # number of latents to be saved
        img_size=None,
        with_synonyms=False,
        with_flowedit=True,
    ):
        self.metadata = metadata
        self.deconceptmeta = deconceptmeta
        self.save_dir = save_dir
        self.size = size
        self.pipe = pipe
        self.seed = seed
        self.device = device
        self.num_inference_steps = num_inference_steps
        self.df = None
        self.concept = concept
        self.neutral_concept = neutral_concept
        self.only_deconcept_latent = only_deconcept_latent
        self.keep_old = keep_old
        self.style = style
        self.num_saved_latent = num_saved_latents
        self.img_size = img_size
        self.with_synonyms = with_synonyms
        self.with_flowedit = with_flowedit

        if with_flowedit:
            from diffusers import FluxPipeline

            pipe_flowedit = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-dev", torch_dtype=torch.bfloat16).to(
                device
            )
            self.pipe_flowedit = pipe_flowedit

        self._validity_check()
        self.prepare_metadata()

        if with_flowedit:
            del self.pipe_flowedit

    def _validity_check(self):
        if not os.path.exists(self.save_dir):
            print(f"save_dir {self.save_dir} does not exist, creating the directory")
            os.makedirs(self.save_dir)

        if not self.keep_old:
            shutil.rmtree(self.save_dir)
            os.mkdir(self.save_dir)

        if not os.path.exists(self.metadata):
            base_dir = os.path.dirname(self.metadata)
            if not os.path.exists(base_dir):
                os.mkdir(base_dir)
            print(f"save_dir {self.metadata} does not exist, downloading the meta data ...")
            if "gcc" in self.metadata:
                url = METADICT["gcc"]
                gdown.download(url, self.metadata, fuzzy=True)
            else:
                raise ValueError("metadata not found, please provide the correct metadata path or download link")

    def _generate_data_with_synonyms(self, template_list):
        """
        Replace parts of the concept with synonyms and apply templates to generate data.
        Returns:
            list: A list of data generated by applying templates to the concept and its synonyms.
        """
        global SYNONYMS_DICT
        # Get all keys from the synonyms dictionary
        synonyms_keys = [*SYNONYMS_DICT.keys()]

        # Create a list starting with the original concept
        concept_list = [self.concept]

        # Replace the first matching key in the concept with its synonyms
        for key in synonyms_keys:
            if key in self.concept:
                synonym_list = SYNONYMS_DICT[key]
                concept_list.extend(self.concept.replace(key, synonym) for synonym in synonym_list)
                break

        # Generate data by applying templates to the concept list
        data = []
        num_concepts = len(concept_list)

        for index, template in enumerate(template_list):
            pos = index % num_concepts
            data.append(template(concept_list[pos]))

        return data

    def _load_and_merge_metadata(self):
        global SIMPLE_DECONCEPT_TEMPLATES, SIMPLE_DESTYLE_TEMPLATES, NSFW_TEMPLATES
        # load concept to template and convert to df
        if self.style == "style":
            deconcept_template = SIMPLE_DESTYLE_TEMPLATES
        elif self.style == "concept":
            deconcept_template = SIMPLE_DECONCEPT_TEMPLATES
        elif self.style == "nsfw":
            deconcept_template = NSFW_TEMPLATES
        else:
            raise ValueError("style should be either concept or style, change the config setting")

        if self.with_synonyms:
            data = self._generate_data_with_synonyms(deconcept_template)
        else:
            data = [t(self.concept) for t in deconcept_template]

        deconceptdf = pd.DataFrame(data, columns=["prompt"])
        # add value to deconceptdf
        deconceptdf["value"] = 1
        deconceptdf = deconceptdf.iloc[: self.size, :]

        # load metadata for reconstruction
        df = pd.read_csv(self.metadata, sep="\t")
        # delete the last column
        df = df.iloc[: self.size, :-1]
        df["value"] = 0
        # rename the first column to prompt without knowing the column name
        df.rename(columns={df.columns[0]: "prompt"}, inplace=True)

        # concatenate the two dataframes and reset the index
        self.df = pd.concat([df, deconceptdf], ignore_index=True)

        assert len(deconceptdf) == len(df), "metadata length mismatch, reduce the data.size in config file"

        # get every second row
        self.df.iloc[::2, :] = deconceptdf
        self.df.iloc[1::2, :] = df

    def _generate_latents(self, prompt, num_inference_steps):
        g_cpu = torch.Generator(self.device).manual_seed(self.seed)
        preparation_phase_output = self.pipe.inference_preparation_phase(
            prompt,
            generator=g_cpu,
            num_inference_steps=num_inference_steps,
            output_type="latent",
            width=self.img_size,
            height=self.img_size,
        )
        intermediate_latents = [preparation_phase_output.latents]
        timesteps = preparation_phase_output.timesteps
        for timesteps_idx, time in enumerate(timesteps):
            latents = self.pipe.inference_denoising_step(timesteps_idx, time, preparation_phase_output)
            preparation_phase_output.latents = latents
            intermediate_latents.append(latents)
        assert len(intermediate_latents) == num_inference_steps + 1, "Intermediate latents length mismatch"
        return intermediate_latents, preparation_phase_output

    def _generate_image(self, intermediate_latents, preparation_phase_output):
        prompt_embeds = preparation_phase_output.prompt_embeds
        g_cpu = torch.Generator(self.device).manual_seed(self.seed)
        img = self.pipe.inference_aft_denoising(
            intermediate_latents[-1], prompt_embeds, g_cpu, "pil", True, self.device
        )
        return img

    def _initialize_save_paths(self):
        self.ptpaths, self.imgpaths, self.idxlist = [], [], []
        self.size = len(self.df)
        for i in range(self.size):
            self.ptpaths.append(os.path.join(self.save_dir, f"{i}.pt"))
            self.imgpaths.append(os.path.join(self.save_dir, f"{i}.png"))
            self.idxlist.append(i)

    def _contain_concept(self, prompt):
        global CON_DECON_DICT
        for k, v in CON_DECON_DICT.items():
            # k is the concept, v is the neutral concept
            if k in prompt:
                return {"concept": k, "neutral_concept": v}
        return None

    @torch.no_grad()
    def prepare_metadata(self):
        global CON_DECON_DICT
        self._load_and_merge_metadata()
        self._initialize_save_paths()
        # save latent tensor
        print("Generating latent tensors and images ...")
        with tqdm.tqdm(total=len(self.df)) as pbar:
            for p, i, idx in zip(self.ptpaths, self.imgpaths, self.idxlist):
                prompt = self.df["prompt"][idx]
                concept_neutral_concet_dict = self._contain_concept(prompt)

                # prompt w concept -> prompt w/o concept if only_deconcept_latent
                if self.only_deconcept_latent and concept_neutral_concet_dict is not None:
                    intermediate_latents, preparation_phase_output = self._generate_latents(
                        prompt, self.num_inference_steps
                    )
                    img = self._generate_image(intermediate_latents, preparation_phase_output)
                    image_tensor = torch.stack(intermediate_latents, dim=0).squeeze(1)
                    img["images"][0].save(i)

                    # neutralize the concept
                    deconcept_prompt = prompt.replace(
                        concept_neutral_concet_dict["concept"], concept_neutral_concet_dict["neutral_concept"]
                    )

                    if self.with_flowedit:
                        img = img["images"][0]
                        img_src = self.pipe_flowedit.image_processor.preprocess(img).to(self.device)
                        x0_src_denorm = self.pipe_flowedit.vae.encode(img_src.to(torch.bfloat16)).latent_dist.mode()
                        x0_src = (
                            x0_src_denorm - self.pipe_flowedit.vae.config.shift_factor
                        ) * self.pipe_flowedit.vae.config.scaling_factor
                        scheduler = self.pipe_flowedit.scheduler
                        x0_tar, last_latent = FlowEditFLUX(
                            self.pipe_flowedit,
                            scheduler,
                            x0_src,
                            prompt,
                            tar_prompt=deconcept_prompt,
                            T_steps=28,
                            n_avg=1,
                            src_guidance_scale=1.5,
                            tar_guidance_scale=5.5,
                            n_min=0,
                            n_max=24,
                        )
                        x0_tar_denorm = (
                            x0_tar / self.pipe_flowedit.vae.config.scaling_factor
                        ) + self.pipe_flowedit.vae.config.shift_factor
                        img_tar = self.pipe.vae.decode(x0_tar_denorm, return_dict=False)[0]
                        img_tar = self.pipe.image_processor.postprocess(img_tar)[0]
                        deconcept_image_tensor = torch.stack([last_latent], dim=0).squeeze(1)

                        # save deconcept image
                        basename = "deconcept_" + os.path.basename(i)
                        i = os.path.join(os.path.dirname(i), basename)
                        img_tar.save(i)

                    else:
                        intermediate_latents_deconcept, preparation_phase_output_deconcept = self._generate_latents(
                            deconcept_prompt, self.num_inference_steps
                        )
                        img_deconcept = self._generate_image(
                            intermediate_latents_deconcept, preparation_phase_output_deconcept
                        )
                        deconcept_image_tensor = torch.stack(intermediate_latents_deconcept, dim=0).squeeze(1)

                        # ensure the inital gaussian noise is the same for both concept and deconcept
                        assert intermediate_latents[0].equal(intermediate_latents_deconcept[0]), "Latent mismatch"

                        # save deconcept image
                        basename = "deconcept_" + os.path.basename(i)
                        i = os.path.join(os.path.dirname(i), basename)
                        img_deconcept["images"][0].save(i)
                else:
                    intermediate_latents, preparation_phase_output = self._generate_latents(
                        prompt, self.num_inference_steps
                    )
                    # use the denoised latent z_o for generating images
                    img = self._generate_image(intermediate_latents, preparation_phase_output)
                    deconcept_image_tensor = torch.tensor([]).to(self.device)
                    image_tensor = torch.stack(intermediate_latents, dim=0).squeeze(1)
                    img["images"][0].save(i)

                # save latent tensors and images
                tmp_image_tensor = {
                    "latents": image_tensor[-self.num_saved_latent :],
                    "deconcept_latents": deconcept_image_tensor[-self.num_saved_latent :],
                }

                save_file(tmp_image_tensor, p)
                pbar.update()

    def __len__(self):
        if self.df is None:
            return 0
        return len(self.df)

    def _load_safetenors(self, path):
        latents = {}
        with safe_open(path, framework="pt") as f:
            for k in f.keys():
                latents[k] = f.get_tensor(k)
        return latents

    def __getitem__(self, idx):
        if self.df is None:
            raise ValueError("metadata is not prepared")
        example = {}
        latents = self._load_safetenors(os.path.join(self.save_dir, f"{idx}.pt"))
        example["image"] = latents["latents"].to(self.device)
        example["deconcept_image"] = latents["deconcept_latents"].to(self.device)
        # load safetensor
        example["prompt"] = self.df["prompt"][idx]
        example["value"] = self.df["value"][idx]
        example["path"] = self.imgpaths[idx]
        return example
