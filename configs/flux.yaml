data:
  save_dir: datasets/${trainer.model}/${logger.project}/${logger.notes}
  metadata: datasets/gcc3m/Validation_GCC-1.1.0-Validation.tsv
  deconceptmeta: configs/concept_long.yaml #Â need to merge all the concepts in one config file
  only_deconcept_latent: true
  size: 40
  batch_size: 1
  concept: "man with a gun"
  style: "concept" # [concept, style, nsfw] concept for deconcept and style for style remove
  filter_ratio: 0.9
  with_fg_filter: false
  with_synonyms: false

trainer:
  epochs: 5
  beta: 0.1
  epsilon: 0.0
  lr: 0.5
  attn_lr: 0
  ff_lr: 0.5
  n_lr: 0.5
  model: flux # [sd1, sd2, sdxl, sd3, flux, dit]
  device: "cuda:0"
  num_intervention_steps: 5
  seed: 48
  init_lambda: 3
  regex: ".*" # "^(down_blocks.[1,2]).*" # optional are ^(down_blocks).*, ^(up_blocks).*, .* (for all heads)
  attn_name: attn # use to filter the attention heads, e.g. attn2 only for cross attention
  head_num_filter: 1 # number of heads to filter, apply lambda to the layter that has more than head_num_filter heads
  masking: "hard_discrete" # [sigmoid, hard_discrete]
  masking_eps: 0.5 # default 0.5
  use_log: false
  precision: 'bf16' # [fp16, bf16]
  disable_progress_bar: true
  accumulate_grad_batches: 4 # batch size
  grad_checkpointing: true # prefered for large models or large num_intervention_steps

lr_scheduler:
  type: "constant" # [linear, cosine, cosine_with_restarts, constant, polynomial]
  warmup_steps: 10 # helps to start with a lower learning rate
  num_cycles: 1
  power: 1.0
  decay_steps: 0

loss:
  reg: 1 # 2 for L2 norm, 1 for L1 norm, 0 for L0 norm
  reconstruct: 2 # 2 for L2 norm, 1 for L1 norm
  mean: true
  use_attn_reg: true
  use_ffn_reg: true
  lambda_reg: true
  reg_alpha: 0.4
  reg_beta: 1 # no need to use beta for now for testing

logger:
  output_dir: "results"
  log_dir: "csv"
  type: "wandb" # [wandb, csv]
  plot_interval: 4
  project: "jan23_recreate_concept_${data.concept}"
  notes: model_${trainer.model}_concept_${data.concept}_eps_${trainer.masking_eps}_beta_${trainer.beta}_sample_${data.size}_epochs_${trainer.epochs}_lr_${trainer.attn_lr}${trainer.ff_lr}${trainer.n_lr}
  tags:
    - "large_blocks_lambda_only"
    - "beta_${trainer.beta}"
    - "recon_norm_${loss.reconstruct}"
    - "reg_norm_${loss.reg}"
  save_lambda_path:
    attn: latest_lambda.pt
    ffn: ${logger.save_lambda_path.attn}
    norm: ${logger.save_lambda_path.attn}

accelerator:
  mixed_precision: False
  report_to: ${logger.type}
  accumulate_grad_batches: ${trainer.accumulate_grad_batches}

debug: true
debug_cfg:
  logger.plot_interval: 4
  trainer.num_intervention_steps: 5
  logger.type: "csv"
